{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NinaPro EMG Spiking Neural Network Classification ===\n",
      "\n",
      "1. Loading data...\n",
      "Available keys in the data: ['__header__', '__version__', '__globals__', 'repetition', 'repetition_object', 'acc', 'daytesting', 'emg', 'object', 'reobject', 'rerepetition', 'restimulus', 'stimulus', 'subj', 'time']\n",
      "EMG data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "\n",
      "2. Preprocessing data for SNN...\n",
      "Spike data shape: (8176, 50, 64)\n",
      "Labels shape: (8176,)\n",
      "Number of classes: 7\n",
      "Average spike rate: 0.8702\n",
      "\n",
      "3. Splitting data...\n",
      "Train set: 5725 samples\n",
      "Validation set: 1224 samples\n",
      "Test set: 1227 samples\n",
      "\n",
      "4. Creating SNN model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling LIFNeuron.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'lif_layer_0' (of type LIFNeuron). Either the `LIFNeuron.call()` method is incorrect, or you need to implement the `LIFNeuron.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\n'SymbolicTensor' object cannot be interpreted as an integer\u001b[0m\n\nArguments received by LIFNeuron.call():\n  • args=('<KerasTensor shape=(None, 50, 64), dtype=float32, sparse=False, ragged=False, name=spike_input>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 531\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history, label_encoder\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 531\u001b[0m     model, history, label_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 459\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4. Creating SNN model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    458\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (time_steps, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 459\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_snn_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreadout_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    466\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# 7. 스파이크 패턴 시각화\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 287\u001b[0m, in \u001b[0;36mcreate_snn_model\u001b[0;34m(input_shape, num_classes, hidden_units, tau, threshold, readout_method)\u001b[0m\n\u001b[1;32m    285\u001b[0m x \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, units \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(hidden_units):\n\u001b[0;32m--> 287\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mLIFNeuron\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlif_layer_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;66;03m# 드롭아웃 추가 (스파이크에 적용)\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(hidden_units) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# 마지막 레이어가 아닌 경우\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[8], line 195\u001b[0m, in \u001b[0;36mLIFNeuron.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    192\u001b[0m refractory_counter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros((batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits))\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# 시간 스텝별 처리\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtime_steps\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    196\u001b[0m     current_input \u001b[38;5;241m=\u001b[39m inputs[:, t, :]  \u001b[38;5;66;03m# (batch_size, input_dim)\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# 시냅스 전류 계산\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling LIFNeuron.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'lif_layer_0' (of type LIFNeuron). Either the `LIFNeuron.call()` method is incorrect, or you need to implement the `LIFNeuron.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\n'SymbolicTensor' object cannot be interpreted as an integer\u001b[0m\n\nArguments received by LIFNeuron.call():\n  • args=('<KerasTensor shape=(None, 50, 64), dtype=float32, sparse=False, ragged=False, name=spike_input>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 데이터 경로 설정\n",
    "DATA_PATH = \"../../data/DB6/DB6_s1_a/S1_D1_T1.mat\"\n",
    "\n",
    "def load_ninapro_data(file_path):\n",
    "    \"\"\"\n",
    "    NinaPro 데이터를 로드하고 전처리하는 함수\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # .mat 파일 로드\n",
    "        data = sio.loadmat(file_path)\n",
    "        \n",
    "        # 일반적인 NinaPro DB6 구조에 따른 키 확인\n",
    "        print(\"Available keys in the data:\", list(data.keys()))\n",
    "        \n",
    "        # 일반적으로 사용되는 키들 (실제 데이터에 따라 조정 필요)\n",
    "        if 'emg' in data:\n",
    "            emg_data = data['emg']\n",
    "        elif 'data' in data:\n",
    "            emg_data = data['data']\n",
    "        else:\n",
    "            # 키를 찾아서 EMG 데이터 추출\n",
    "            data_keys = [k for k in data.keys() if not k.startswith('__')]\n",
    "            emg_data = data[data_keys[0]]\n",
    "        \n",
    "        if 'stimulus' in data:\n",
    "            labels = data['stimulus'].flatten()\n",
    "        elif 'restimulus' in data:\n",
    "            labels = data['restimulus'].flatten()\n",
    "        elif 'glove' in data:\n",
    "            labels = data['glove']\n",
    "            if labels.ndim > 1:\n",
    "                labels = labels[:, 0]  # 첫 번째 열 사용\n",
    "        else:\n",
    "            # 라벨 데이터 찾기\n",
    "            label_keys = [k for k in data.keys() if 'stimulus' in k.lower() or 'label' in k.lower()]\n",
    "            if label_keys:\n",
    "                labels = data[label_keys[0]].flatten()\n",
    "            else:\n",
    "                # 두 번째로 큰 배열을 라벨로 가정\n",
    "                data_keys = [k for k in data.keys() if not k.startswith('__')]\n",
    "                labels = data[data_keys[1]].flatten() if len(data_keys) > 1 else np.zeros(emg_data.shape[0])\n",
    "        \n",
    "        print(f\"EMG data shape: {emg_data.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Unique labels: {np.unique(labels)}\")\n",
    "        \n",
    "        return emg_data, labels\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        # 샘플 데이터 생성 (실제 데이터가 없을 경우)\n",
    "        print(\"Generating sample data for demonstration...\")\n",
    "        n_samples = 10000\n",
    "        n_channels = 12\n",
    "        emg_data = np.random.randn(n_samples, n_channels) * 0.1\n",
    "        # EMG 신호처럼 보이도록 노이즈 추가\n",
    "        for i in range(n_channels):\n",
    "            emg_data[:, i] += np.sin(np.linspace(0, 100*np.pi, n_samples)) * 0.05\n",
    "        labels = np.random.randint(0, 7, n_samples)  # 0-6 클래스\n",
    "        return emg_data, labels\n",
    "\n",
    "def emg_to_spike_encoding(emg_data, threshold_method='adaptive', time_steps=50):\n",
    "    \"\"\"\n",
    "    EMG 신호를 스파이크 트레인으로 변환하는 함수\n",
    "    \"\"\"\n",
    "    n_samples, n_channels = emg_data.shape\n",
    "    spike_data = np.zeros((n_samples, time_steps, n_channels))\n",
    "    \n",
    "    for sample_idx in range(n_samples):\n",
    "        for channel_idx in range(n_channels):\n",
    "            signal = emg_data[sample_idx, channel_idx]\n",
    "            \n",
    "            if threshold_method == 'adaptive':\n",
    "                # 적응적 임계값 설정\n",
    "                threshold = np.std(signal) * 2.0\n",
    "            elif threshold_method == 'fixed':\n",
    "                # 고정 임계값\n",
    "                threshold = 0.1\n",
    "            else:\n",
    "                # 백분위수 기반\n",
    "                threshold = np.percentile(np.abs(signal), 75)\n",
    "            \n",
    "            # Rate coding: 신호 강도에 비례한 스파이크 빈도\n",
    "            spike_rate = np.abs(signal) / (threshold + 1e-8)\n",
    "            spike_rate = np.clip(spike_rate, 0, 1)\n",
    "            \n",
    "            # 포아송 과정으로 스파이크 생성\n",
    "            spikes = np.random.random(time_steps) < spike_rate\n",
    "            spike_data[sample_idx, :, channel_idx] = spikes.astype(np.float32)\n",
    "    \n",
    "    return spike_data\n",
    "\n",
    "def preprocess_data_for_snn(emg_data, labels, window_size=200, overlap=100, time_steps=50):\n",
    "    \"\"\"\n",
    "    SNN을 위한 EMG 데이터 전처리\n",
    "    \"\"\"\n",
    "    # 레이블이 0인 rest 구간 제거 (선택사항)\n",
    "    non_zero_mask = labels != 0\n",
    "    emg_data = emg_data[non_zero_mask]\n",
    "    labels = labels[non_zero_mask]\n",
    "    \n",
    "    # 윈도우 기반 시퀀스 생성\n",
    "    windowed_sequences = []\n",
    "    windowed_labels = []\n",
    "    \n",
    "    step_size = window_size - overlap\n",
    "    \n",
    "    for i in range(0, len(emg_data) - window_size + 1, step_size):\n",
    "        window = emg_data[i:i+window_size]\n",
    "        window_label = labels[i:i+window_size]\n",
    "        \n",
    "        # 윈도우 내에서 가장 빈번한 라벨 사용\n",
    "        unique_labels, counts = np.unique(window_label, return_counts=True)\n",
    "        dominant_label = unique_labels[np.argmax(counts)]\n",
    "        \n",
    "        # 윈도우를 요약하여 하나의 특징 벡터로 변환\n",
    "        features = []\n",
    "        for channel in range(window.shape[1]):\n",
    "            channel_data = window[:, channel]\n",
    "            features.extend([\n",
    "                np.mean(channel_data),\n",
    "                np.std(channel_data),\n",
    "                np.max(channel_data) - np.min(channel_data),  # 범위\n",
    "                np.mean(np.abs(np.diff(channel_data))),       # 평균 변화율\n",
    "            ])\n",
    "        \n",
    "        windowed_sequences.append(features)\n",
    "        windowed_labels.append(dominant_label)\n",
    "    \n",
    "    # EMG 특징을 스파이크로 인코딩\n",
    "    emg_features = np.array(windowed_sequences)\n",
    "    spike_data = emg_to_spike_encoding(emg_features, time_steps=time_steps)\n",
    "    \n",
    "    return spike_data, np.array(windowed_labels)\n",
    "\n",
    "class LIFNeuron(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Leaky Integrate-and-Fire (LIF) 뉴런 구현\n",
    "    \"\"\"\n",
    "    def __init__(self, units, tau=20.0, threshold=1.0, reset_value=0.0, \n",
    "                 refractory_period=2, **kwargs):\n",
    "        super(LIFNeuron, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.tau = tau  # 막 시정수\n",
    "        self.threshold = threshold  # 스파이크 임계값\n",
    "        self.reset_value = reset_value  # 리셋 전압\n",
    "        self.refractory_period = refractory_period  # 불응기\n",
    "        \n",
    "        # 학습 가능한 가중치\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # 가중치 초기화\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='glorot_uniform',\n",
    "            name='lif_weights',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros',\n",
    "            name='lif_bias',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(LIFNeuron, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        time_steps = tf.shape(inputs)[1]\n",
    "        \n",
    "        # 상태 변수 초기화\n",
    "        membrane_potential = tf.zeros((batch_size, self.units))\n",
    "        output_spikes = tf.TensorArray(tf.float32, size=time_steps)\n",
    "        refractory_counter = tf.zeros((batch_size, self.units))\n",
    "        \n",
    "        # 시간 스텝별 처리\n",
    "        for t in range(time_steps):\n",
    "            current_input = inputs[:, t, :]  # (batch_size, input_dim)\n",
    "            \n",
    "            # 시냅스 전류 계산\n",
    "            synaptic_current = tf.matmul(current_input, self.w) + self.b\n",
    "            \n",
    "            # 불응기 체크\n",
    "            not_refractory = tf.cast(refractory_counter <= 0, tf.float32)\n",
    "            \n",
    "            # 막전위 업데이트 (LIF 동역학)\n",
    "            membrane_potential = (\n",
    "                membrane_potential * (1 - 1/self.tau) + \n",
    "                synaptic_current * not_refractory\n",
    "            )\n",
    "            \n",
    "            # 스파이크 생성\n",
    "            spikes = tf.cast(membrane_potential >= self.threshold, tf.float32)\n",
    "            \n",
    "            # 막전위 리셋\n",
    "            membrane_potential = tf.where(\n",
    "                spikes > 0, \n",
    "                tf.ones_like(membrane_potential) * self.reset_value,\n",
    "                membrane_potential\n",
    "            )\n",
    "            \n",
    "            # 불응기 카운터 업데이트\n",
    "            refractory_counter = tf.where(\n",
    "                spikes > 0,\n",
    "                tf.ones_like(refractory_counter) * self.refractory_period,\n",
    "                tf.maximum(refractory_counter - 1, 0)\n",
    "            )\n",
    "            \n",
    "            output_spikes = output_spikes.write(t, spikes)\n",
    "        \n",
    "        # 출력 조합\n",
    "        spike_output = tf.transpose(output_spikes.stack(), [1, 0, 2])  # (batch, time, units)\n",
    "        \n",
    "        return spike_output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "            'tau': self.tau,\n",
    "            'threshold': self.threshold,\n",
    "            'reset_value': self.reset_value,\n",
    "            'refractory_period': self.refractory_period,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class SpikeReadout(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    스파이크를 최종 출력으로 변환하는 레이어\n",
    "    \"\"\"\n",
    "    def __init__(self, readout_method='rate', **kwargs):\n",
    "        super(SpikeReadout, self).__init__(**kwargs)\n",
    "        self.readout_method = readout_method\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        if self.readout_method == 'rate':\n",
    "            # 스파이크 빈도를 계산\n",
    "            return tf.reduce_mean(inputs, axis=1)  # 시간 축 평균\n",
    "        elif self.readout_method == 'sum':\n",
    "            # 총 스파이크 수\n",
    "            return tf.reduce_sum(inputs, axis=1)\n",
    "        elif self.readout_method == 'last':\n",
    "            # 마지막 시간 스텝\n",
    "            return inputs[:, -1, :]\n",
    "        else:\n",
    "            # 가중 평균 (최근 시간에 더 높은 가중치)\n",
    "            time_steps = tf.shape(inputs)[1]\n",
    "            weights = tf.linspace(0.1, 1.0, time_steps)\n",
    "            weights = weights / tf.reduce_sum(weights)\n",
    "            return tf.reduce_sum(inputs * weights[None, :, None], axis=1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'readout_method': self.readout_method})\n",
    "        return config\n",
    "\n",
    "def create_snn_model(input_shape, num_classes, hidden_units=[128, 64], \n",
    "                     tau=20.0, threshold=1.0, readout_method='rate'):\n",
    "    \"\"\"\n",
    "    Spiking Neural Network 모델 생성\n",
    "    \"\"\"\n",
    "    time_steps, n_features = input_shape\n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='spike_input')\n",
    "    \n",
    "    # SNN 레이어들\n",
    "    x = inputs\n",
    "    for i, units in enumerate(hidden_units):\n",
    "        x = LIFNeuron(\n",
    "            units=units,\n",
    "            tau=tau,\n",
    "            threshold=threshold,\n",
    "            name=f'lif_layer_{i}'\n",
    "        )(x)\n",
    "        \n",
    "        # 드롭아웃 추가 (스파이크에 적용)\n",
    "        if i < len(hidden_units) - 1:  # 마지막 레이어가 아닌 경우\n",
    "            x = Dropout(0.2)(x)\n",
    "    \n",
    "    # 출력 LIF 레이어\n",
    "    output_spikes = LIFNeuron(\n",
    "        units=num_classes,\n",
    "        tau=tau * 0.5,  # 출력 레이어는 더 빠른 시정수\n",
    "        threshold=threshold * 0.8,\n",
    "        name='output_lif'\n",
    "    )(x)\n",
    "    \n",
    "    # 스파이크를 최종 출력으로 변환\n",
    "    readout = SpikeReadout(readout_method=readout_method)(output_spikes)\n",
    "    \n",
    "    # 소프트맥스 적용\n",
    "    outputs = tf.keras.activations.softmax(readout)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='SNN_EMG_Classifier')\n",
    "    \n",
    "    # 모델 컴파일\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def plot_spike_raster(spike_data, sample_idx=0, max_neurons=20):\n",
    "    \"\"\"\n",
    "    스파이크 래스터 플롯 생성\n",
    "    \"\"\"\n",
    "    sample_spikes = spike_data[sample_idx]  # (time_steps, n_channels)\n",
    "    time_steps, n_channels = sample_spikes.shape\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for neuron in range(min(n_channels, max_neurons)):\n",
    "        spike_times = np.where(sample_spikes[:, neuron] > 0)[0]\n",
    "        plt.scatter(spike_times, [neuron] * len(spike_times), \n",
    "                   s=2, alpha=0.7, label=f'Channel {neuron}' if neuron < 5 else \"\")\n",
    "    \n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('EMG Channel')\n",
    "    plt.title(f'Spike Raster Plot - Sample {sample_idx}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if max_neurons <= 5:\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    SNN 학습 과정 시각화\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 정확도 플롯\n",
    "    axes[0, 0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    axes[0, 0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[0, 0].set_title('SNN Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 손실 플롯\n",
    "    axes[0, 1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[0, 1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0, 1].set_title('SNN Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 마지막 10 에포크 정확도\n",
    "    start_epoch = max(0, len(history.history['accuracy']) - 10)\n",
    "    epochs_range = range(start_epoch, len(history.history['accuracy']))\n",
    "    \n",
    "    axes[1, 0].plot(epochs_range, history.history['accuracy'][start_epoch:], \n",
    "                   label='Training Accuracy', linewidth=2, marker='o')\n",
    "    axes[1, 0].plot(epochs_range, history.history['val_accuracy'][start_epoch:], \n",
    "                   label='Validation Accuracy', linewidth=2, marker='s')\n",
    "    axes[1, 0].set_title('Last 10 Epochs - Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # SNN 특성 정보\n",
    "    axes[1, 1].text(0.5, 0.5, \n",
    "                   'SNN Architecture\\n\\n• LIF Neurons\\n• Spike-based Processing\\n• Temporal Dynamics\\n• Bio-inspired Computing\\n• Event-driven Processing', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes,\n",
    "                   fontsize=11, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.5))\n",
    "    axes[1, 1].set_xlim(0, 1)\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].set_title('SNN Architecture Info', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Spiking Neural Network Training Results', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    혼동 행렬 시각화\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('SNN Model - Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"=== NinaPro EMG Spiking Neural Network Classification ===\")\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    \n",
    "    # 2. SNN용 데이터 전처리\n",
    "    print(\"\\n2. Preprocessing data for SNN...\")\n",
    "    time_steps = 50\n",
    "    X, y = preprocess_data_for_snn(emg_data, labels, \n",
    "                                   window_size=200, overlap=100, \n",
    "                                   time_steps=time_steps)\n",
    "    \n",
    "    print(f\"Spike data shape: {X.shape}\")  # (n_samples, time_steps, n_features)\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "    print(f\"Average spike rate: {np.mean(X):.4f}\")\n",
    "    \n",
    "    # 3. 라벨 인코딩\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    class_names = [f\"Gesture {i}\" for i in range(num_classes)]\n",
    "    \n",
    "    # 4. 데이터 분할\n",
    "    print(\"\\n3. Splitting data...\")\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.15, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # 5. 원-핫 인코딩\n",
    "    y_train_cat = to_categorical(y_train, num_classes)\n",
    "    y_val_cat = to_categorical(y_val, num_classes)\n",
    "    y_test_cat = to_categorical(y_test, num_classes)\n",
    "    \n",
    "    # 6. SNN 모델 생성\n",
    "    print(\"\\n4. Creating SNN model...\")\n",
    "    input_shape = (time_steps, X_train.shape[-1])\n",
    "    model = create_snn_model(\n",
    "        input_shape=input_shape,\n",
    "        num_classes=num_classes,\n",
    "        hidden_units=[128, 64],\n",
    "        tau=20.0,\n",
    "        threshold=1.0,\n",
    "        readout_method='rate'\n",
    "    )\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    # 7. 스파이크 패턴 시각화\n",
    "    print(\"\\n5. Visualizing spike patterns...\")\n",
    "    plot_spike_raster(X_train, sample_idx=0, max_neurons=12)\n",
    "    \n",
    "    # 8. 콜백 설정\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_accuracy', patience=25, restore_best_weights=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-6, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    # 9. 모델 학습\n",
    "    print(\"\\n6. Training SNN model for 100 epochs...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        batch_size=32,\n",
    "        epochs=100,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 10. 최종 평가\n",
    "    print(\"\\n7. Evaluating SNN model...\")\n",
    "    \n",
    "    # 검증 세트 평가\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val_cat, verbose=0)\n",
    "    print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # 테스트 세트 평가\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # 예측\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # 11. 결과 시각화\n",
    "    print(\"\\n8. Plotting results...\")\n",
    "    \n",
    "    # 학습 과정 시각화\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # 혼동 행렬 시각화\n",
    "    plot_confusion_matrix(y_test, y_pred, class_names)\n",
    "    \n",
    "    # 분류 리포트\n",
    "    print(\"\\n9. Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "    \n",
    "    # 최종 결과 요약\n",
    "    print(\"\\n=== Final SNN Results Summary ===\")\n",
    "    print(f\"Training Accuracy: {max(history.history['accuracy']):.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Total Epochs Trained: {len(history.history['accuracy'])}\")\n",
    "    print(f\"Model Parameters: {model.count_params():,}\")\n",
    "    print(f\"Average Spike Rate: {np.mean(X_train):.4f}\")\n",
    "    \n",
    "    return model, history, label_encoder\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, history, label_encoder = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
