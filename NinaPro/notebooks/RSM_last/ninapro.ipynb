{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b01d87a-7a83-4a09-86a5-a7d51c17a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Unique labels: [ 0  1  3  4  6  9 10 11]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 393\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtcn_ninapro_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 345\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    340\u001b[0m train_data, val_data, train_labels, val_labels \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m    341\u001b[0m     train_data, train_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mtrain_labels\n\u001b[1;32m    342\u001b[0m )\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNinaProDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWINDOW_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m NinaProDataset(val_data, val_labels, WINDOW_SIZE)\n\u001b[1;32m    347\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m NinaProDataset(test_data, test_labels, WINDOW_SIZE)\n",
      "Cell \u001b[0;32mIn[13], line 116\u001b[0m, in \u001b[0;36mNinaProDataset.__init__\u001b[0;34m(self, data, labels, window_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m=\u001b[39m window_size\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Create sliding windows\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 132\u001b[0m, in \u001b[0;36mNinaProDataset._create_windows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m         windows\u001b[38;5;241m.\u001b[39mappend(window)\n\u001b[1;32m    130\u001b[0m         window_labels\u001b[38;5;241m.\u001b[39mappend(label \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert to 0-based indexing\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindows\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(window_labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        \n",
    "        for i in range(len(self.data) - self.window_size + 1):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size // 2]\n",
    "            \n",
    "            # Skip rest periods (label 0 if using 0-based indexing)\n",
    "            if label > 0:  # Adjust based on your labeling scheme\n",
    "                windows.append(window)\n",
    "                window_labels.append(label - 1)  # Convert to 0-based indexing\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.windows[idx]), torch.LongTensor([self.window_labels[idx]])[0]\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Unique labels: {np.unique(labels)}\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    # Split data\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        emg_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE)\n",
    "    val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE)\n",
    "    test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "    num_classes = len(np.unique(labels)) - 1  # Excluding rest class\n",
    "    \n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3387da1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Original unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "Gesture labels (excluding rest): [ 1  3  4  6  9 10 11]\n",
      "Number of gesture classes: 7\n",
      "Label mapping: {1: 0, 3: 1, 4: 2, 6: 3, 9: 4, 10: 5, 11: 6}\n",
      "Window shape example: (200, 16)\n",
      "Window shape example: (200, 16)\n",
      "Window shape example: (200, 16)\n",
      "Training samples: 3434\n",
      "Validation samples: 862\n",
      "Test samples: 1056\n",
      "Model initialized with 16 input features and 7 classes\n",
      "Total parameters: 223815\n",
      "\n",
      "Starting training...\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 428\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, label_mapping\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 401\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 401\u001b[0m train_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[39], line 244\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 244\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[1;32m    246\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 97\u001b[0m, in \u001b[0;36mTCN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Pass through TCN\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_channels[-1])\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 50\u001b[0m, in \u001b[0;36mTemporalBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 50\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     res \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out \u001b[38;5;241m+\u001b[39m res)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200, label_mapping: dict = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.label_mapping = label_mapping\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        print(\"Window shape example:\", self.windows[0].shape)\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        stride = self.window_size // 2\n",
    "\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, stride):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size // 2]\n",
    "            \n",
    "            # Skip rest periods (label 0 in NinaPro)\n",
    "            if label > 0:\n",
    "                windows.append(window)\n",
    "                # Map label to continuous 0-based indexing\n",
    "                mapped_label = self.label_mapping[label] if self.label_mapping else label - 1\n",
    "                window_labels.append(mapped_label)\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.windows[idx]), torch.LongTensor([self.window_labels[idx]])[0]\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, dict, int]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    # Create label mapping for continuous indexing\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Remove rest label (0) if present\n",
    "    gesture_labels = unique_labels[unique_labels > 0]\n",
    "    \n",
    "    # Create mapping from original labels to 0-based continuous labels\n",
    "    label_mapping = {label: idx for idx, label in enumerate(gesture_labels)}\n",
    "    num_classes = len(gesture_labels)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Original unique labels: {unique_labels}\")\n",
    "    print(f\"Gesture labels (excluding rest): {gesture_labels}\")\n",
    "    print(f\"Number of gesture classes: {num_classes}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    return emg_data, labels, label_mapping, num_classes\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            print(f\"Batch Data shape: {batch_data.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    TEST_DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T2.mat\" \n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128] \n",
    "    #[64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels, label_mapping, num_classes = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    # Split data\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        emg_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    train_data = train_data[:len(train_data) // 2]\n",
    "    train_labels = train_labels[:len(train_labels) // 2] \n",
    "    val_data = val_data[:len(val_data) // 2]\n",
    "    val_labels = val_labels[:len(val_labels) // 2]  \n",
    "    test_data = test_data[:len(test_data) // 2]\n",
    "    test_labels = test_labels[:len(test_labels) // 2]  \n",
    "    \n",
    "    # Create datasets with label mapping\n",
    "    train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE, label_mapping)\n",
    "    val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE, label_mapping)\n",
    "    test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE, label_mapping)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "     \n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "    \n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model and label mapping\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'label_mapping': label_mapping,\n",
    "        'num_classes': num_classes,\n",
    "        'num_features': num_features\n",
    "    }, 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "    \n",
    "    return model, label_mapping\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d808522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Original unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "Gesture labels (excluding rest): [ 1  3  4  6  9 10 11]\n",
      "Number of gesture classes: 7\n",
      "Label mapping: {1: 0, 3: 1, 4: 2, 6: 3, 9: 4, 10: 5, 11: 6}\n",
      "Window shape example: (16,)\n",
      "Window shape example: (16,)\n",
      "Window shape example: (16,)\n",
      "Training loader shape: 2\n",
      "Training samples: 3427\n",
      "Validation samples: 857\n",
      "Test samples: 1057\n",
      "Model initialized with 16 input features and 7 classes\n",
      "Total parameters: 223815\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 441\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, label_mapping\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 414\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 414\u001b[0m train_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 256\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    253\u001b[0m batch_data, batch_labels \u001b[38;5;241m=\u001b[39m batch_data\u001b[38;5;241m.\u001b[39mto(device), batch_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    255\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 256\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[1;32m    258\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 99\u001b[0m, in \u001b[0;36mTCN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m) \n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Pass through TCN\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n\u001b[1;32m    102\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_channels[-1])\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 52\u001b[0m, in \u001b[0;36mTemporalBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 52\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     res \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out \u001b[38;5;241m+\u001b[39m res)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from scipy.signal import butter, filtfilt\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.unsqueeze(2) \n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200, label_mapping: dict = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.label_mapping = label_mapping\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        print(\"Window shape example:\", self.windows[0].shape)\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        stride = self.window_size // 2\n",
    "\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, stride):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size // 2]\n",
    "            \n",
    "            # Skip rest periods (label 0 in NinaPro)\n",
    "            if label > 0:\n",
    "                rms_features = np.sqrt(np.mean(window ** 2, axis=0))  # shape: (n_features,)\n",
    "                windows.append(rms_features)\n",
    "\n",
    "                mapped_label = self.label_mapping[label] if self.label_mapping else label - 1\n",
    "                window_labels.append(mapped_label)\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.windows[idx]), torch.LongTensor([self.window_labels[idx]])[0]\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "def bandpass_filter(signal, lowcut=20, highcut=450, fs=2000, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal, axis=0)\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, dict, int]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "\n",
    "    #  remove noise\n",
    "    emg_data = bandpass_filter(emg_data, lowcut=20, highcut=450, fs=2000)\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    # Create label mapping for continuous indexing\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Remove rest label (0) if present\n",
    "    gesture_labels = unique_labels[unique_labels > 0]\n",
    "    \n",
    "    # Create mapping from original labels to 0-based continuous labels\n",
    "    label_mapping = {label: idx for idx, label in enumerate(gesture_labels)}\n",
    "    num_classes = len(gesture_labels)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Original unique labels: {unique_labels}\")\n",
    "    print(f\"Gesture labels (excluding rest): {gesture_labels}\")\n",
    "    print(f\"Number of gesture classes: {num_classes}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    return emg_data, labels, label_mapping, num_classes\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    TEST_DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T2.mat\" \n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128] \n",
    "    #[64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels, label_mapping, num_classes = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    # Split data\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        emg_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    train_data = train_data[:len(train_data) // 2]\n",
    "    train_labels = train_labels[:len(train_labels) // 2] \n",
    "    val_data = val_data[:len(val_data) // 2]\n",
    "    val_labels = val_labels[:len(val_labels) // 2]  \n",
    "    test_data = test_data[:len(test_data) // 2]\n",
    "    test_labels = test_labels[:len(test_labels) // 2]  \n",
    "    \n",
    "    # Create datasets with label mapping\n",
    "    train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE, label_mapping)\n",
    "    val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE, label_mapping)\n",
    "    test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE, label_mapping)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "     \n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "    \n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model and label mapping\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'label_mapping': label_mapping,\n",
    "        'num_classes': num_classes,\n",
    "        'num_features': num_features\n",
    "    }, 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "    \n",
    "    return model, label_mapping\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34643fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Original unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "Gesture labels (excluding rest): [ 1  3  4  6  9 10 11]\n",
      "Number of gesture classes: 7\n",
      "Label mapping: {1: 0, 3: 1, 4: 2, 6: 3, 9: 4, 10: 5, 11: 6}\n",
      "Window shape example: (16,)\n",
      "Training loader shape: 2\n",
      "Training samples: 6816\n",
      "Validation samples: 1704\n",
      "Test samples: 2130\n",
      "Num features: 16\n",
      "Model initialized with 16 input features and 7 classes\n",
      "Total parameters: 223815\n",
      "\n",
      "Starting training...\n",
      "Epoch [10/100], Train Loss: 0.9792, Val Accuracy: 65.02%\n",
      "Epoch [20/100], Train Loss: 0.8446, Val Accuracy: 67.19%\n",
      "Epoch [30/100], Train Loss: 0.7646, Val Accuracy: 69.48%\n",
      "Epoch [40/100], Train Loss: 0.7154, Val Accuracy: 70.13%\n",
      "Epoch [50/100], Train Loss: 0.6080, Val Accuracy: 72.42%\n",
      "Epoch [60/100], Train Loss: 0.5975, Val Accuracy: 71.89%\n",
      "Epoch [70/100], Train Loss: 0.5815, Val Accuracy: 72.18%\n",
      "Epoch [80/100], Train Loss: 0.5718, Val Accuracy: 72.65%\n",
      "Epoch [90/100], Train Loss: 0.5658, Val Accuracy: 72.42%\n",
      "Epoch [100/100], Train Loss: 0.5490, Val Accuracy: 72.42%\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Final Test Accuracy: 0.7211\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       305\n",
      "           1       0.62      0.62      0.62       304\n",
      "           2       0.74      0.71      0.73       307\n",
      "           3       0.60      0.61      0.61       298\n",
      "           4       0.71      0.75      0.72       311\n",
      "           5       0.75      0.74      0.75       290\n",
      "           6       0.78      0.80      0.79       315\n",
      "\n",
      "    accuracy                           0.72      2130\n",
      "   macro avg       0.72      0.72      0.72      2130\n",
      "weighted avg       0.72      0.72      0.72      2130\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved as 'tcn_ninapro_model.pth'\n"
     ]
    }
   ],
   "source": [
    "#   NinaDataset  train, test, val dataset split\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from scipy.signal import butter, filtfilt\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.unsqueeze(2) \n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200, label_mapping: dict = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.label_mapping = label_mapping\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        print(\"Window shape example:\", self.windows[0].shape)\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        stride = self.window_size // 2\n",
    "\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, stride):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size - 1]\n",
    "            \n",
    "            # Skip rest periods (label 0 in NinaPro)\n",
    "            if label > 0:\n",
    "                rms_features = np.sqrt(np.mean(window ** 2, axis=0))  # shape: (n_features,)\n",
    "                windows.append(rms_features)\n",
    "\n",
    "                mapped_label = self.label_mapping[label] if self.label_mapping else label - 1\n",
    "                window_labels.append(mapped_label)\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.windows[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.window_labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "def bandpass_filter(signal, lowcut=20, highcut=450, fs=2000, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal, axis=0)\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, dict, int]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "\n",
    "    #  remove noise\n",
    "    emg_data = bandpass_filter(emg_data, lowcut=20, highcut=450, fs=2000)\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    # Create label mapping for continuous indexing\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Remove rest label (0) if present\n",
    "    gesture_labels = unique_labels[unique_labels > 0]\n",
    "    \n",
    "    # Create mapping from original labels to 0-based continuous labels\n",
    "    label_mapping = {label: idx for idx, label in enumerate(gesture_labels)}\n",
    "    num_classes = len(gesture_labels)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Original unique labels: {unique_labels}\")\n",
    "    print(f\"Gesture labels (excluding rest): {gesture_labels}\")\n",
    "    print(f\"Number of gesture classes: {num_classes}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    return emg_data, labels, label_mapping, num_classes\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class EMGWindowDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    TEST_DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T2.mat\" \n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128] \n",
    "    #[64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels, label_mapping, num_classes = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    dataset = NinaProDataset(emg_data, labels, WINDOW_SIZE, label_mapping)\n",
    "    # Split data\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        dataset.windows, dataset.window_labels, test_size=0.2, random_state=42, stratify=dataset.window_labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    # train_data = train_data[:len(train_data) // 2]\n",
    "    # train_labels = train_labels[:len(train_labels) // 2] \n",
    "    # val_data = val_data[:len(val_data) // 2]\n",
    "    # val_labels = val_labels[:len(val_labels) // 2]  \n",
    "    # test_data = test_data[:len(test_data) // 2]\n",
    "    # test_labels = test_labels[:len(test_labels) // 2]  \n",
    "    \n",
    "    # # Create datasets with label mapping\n",
    "    # train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE, label_mapping)\n",
    "    # val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE, label_mapping)\n",
    "    # test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE, label_mapping)\n",
    "\n",
    "    train_dataset = EMGWindowDataset(train_data, train_labels)\n",
    "    val_dataset = EMGWindowDataset(val_data, val_labels)\n",
    "    test_dataset = EMGWindowDataset(test_data, test_labels)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    batch = next(iter(train_loader))\n",
    "    print(f\"Training loader shape: {len(batch)}\")\n",
    "\n",
    "     \n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "\n",
    "    print(f\"Num features: {num_features}\")\n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model and label mapping\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'label_mapping': label_mapping,\n",
    "        'num_classes': num_classes,\n",
    "        'num_features': num_features\n",
    "    }, 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "    \n",
    "    return model, label_mapping\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1df0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Original unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "Gesture labels (excluding rest): [ 1  3  4  6  9 10 11]\n",
      "Number of gesture classes: 7\n",
      "Label mapping: {1: 0, 3: 1, 4: 2, 6: 3, 9: 4, 10: 5, 11: 6}\n",
      "Window shape example: (16,)\n",
      "Training loader shape: 2\n",
      "Training samples: 6816\n",
      "Validation samples: 1704\n",
      "Test samples: 2130\n",
      "Num features: 16\n",
      "Model initialized with 16 input features and 7 classes\n",
      "Total parameters: 223815\n",
      "\n",
      "Starting training...\n",
      "Epoch [10/100], Train Loss: 0.9776, Val Accuracy: 64.26%\n",
      "Epoch [20/100], Train Loss: 0.8525, Val Accuracy: 68.13%\n",
      "Epoch [30/100], Train Loss: 0.7704, Val Accuracy: 70.25%\n",
      "Epoch [40/100], Train Loss: 0.7212, Val Accuracy: 70.01%\n",
      "Epoch [50/100], Train Loss: 0.6788, Val Accuracy: 71.54%\n",
      "Epoch [60/100], Train Loss: 0.6324, Val Accuracy: 72.59%\n",
      "Epoch [70/100], Train Loss: 0.5435, Val Accuracy: 73.18%\n",
      "Epoch [80/100], Train Loss: 0.5186, Val Accuracy: 72.95%\n",
      "Epoch [90/100], Train Loss: 0.5242, Val Accuracy: 73.30%\n",
      "Epoch [100/100], Train Loss: 0.5148, Val Accuracy: 73.42%\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Final Test Accuracy: 0.7272\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84       305\n",
      "           1       0.63      0.63      0.63       304\n",
      "           2       0.77      0.70      0.73       307\n",
      "           3       0.64      0.63      0.64       298\n",
      "           4       0.67      0.77      0.72       311\n",
      "           5       0.76      0.77      0.76       290\n",
      "           6       0.79      0.77      0.78       315\n",
      "\n",
      "    accuracy                           0.73      2130\n",
      "   macro avg       0.73      0.73      0.73      2130\n",
      "weighted avg       0.73      0.73      0.73      2130\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved as 'tcn_ninapro_model.pth'\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "if __name__ == \"__main__\":\n",
    "    model, label_mapping = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b458585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def measure_inference_time(model, input_tensor, device='cpu', repeat=100):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(input_tensor)\n",
    "\n",
    "        start = time.time()\n",
    "        for _ in range(repeat):\n",
    "            _ = model(input_tensor)\n",
    "        end = time.time()\n",
    "\n",
    "    return (end - start) / repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09a424ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Original unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "Gesture labels (excluding rest): [ 1  3  4  6  9 10 11]\n",
      "Number of gesture classes: 7\n",
      "Label mapping: {1: 0, 3: 1, 4: 2, 6: 3, 9: 4, 10: 5, 11: 6}\n",
      "Window shape example: (16,)\n",
      "Inference time: 2.21 ms\n"
     ]
    }
   ],
   "source": [
    " # Configuration\n",
    "DATA_PATH = \"../../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "WINDOW_SIZE = 200\n",
    "BATCH_SIZE = 32\n",
    "    # Load data\n",
    "emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "emg_data, labels, label_mapping, num_classes = preprocess_data(emg_data, labels)\n",
    "    \n",
    "dataset = NinaProDataset(emg_data, labels, WINDOW_SIZE, label_mapping)\n",
    "    # Split data\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        dataset.windows, dataset.window_labels, test_size=0.2, random_state=42, stratify=dataset.window_labels\n",
    ")\n",
    "    \n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    ")\n",
    "    \n",
    "\n",
    "train_dataset = EMGWindowDataset(train_data, train_labels)\n",
    "val_dataset = EMGWindowDataset(val_data, val_labels)\n",
    "test_dataset = EMGWindowDataset(test_data, test_labels)\n",
    "    \n",
    "    # Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for x, y in test_loader:\n",
    "    sample_input = x[0].unsqueeze(0)  # shape: (1, C, L)\n",
    "    sample_label = y[0].unsqueeze(0)  # () \n",
    "    break \n",
    "\n",
    "time_per_inference = measure_inference_time(model, sample_input)\n",
    "print(f\"Inference time: {time_per_inference * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f0644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
